{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CGOvicn1kjWJ"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers accelerate sentencepiece bitsandbytes\n",
        "!pip install -q pillow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "N4oIovaBjA_n"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\shiva\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import pandas as pd\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXk3sDdyiaq8",
        "outputId": "a2ad02dd-906e-496f-a273-c003c93a6840"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        }
      ],
      "source": [
        "model_id = \"google/gemma-3-1b-it\"  # Use \"google/gemma-3b-it\" if you have Colab Pro/A100\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, token=\"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\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    load_in_4bit=True,  # 4-bit quantization to fit in T4/A100\n",
        "    token=\"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\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Lg6wWSbovHk",
        "outputId": "4d8ca4e4-7239-44b6-be15-d1e43ab75d18"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Gemma3ForCausalLM(\n",
              "  (model): Gemma3TextModel(\n",
              "    (embed_tokens): Gemma3TextScaledWordEmbedding(262144, 1152, padding_idx=0)\n",
              "    (layers): ModuleList(\n",
              "      (0-25): 26 x Gemma3DecoderLayer(\n",
              "        (self_attn): Gemma3Attention(\n",
              "          (q_proj): Linear4bit(in_features=1152, out_features=1024, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=1152, out_features=256, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=1152, out_features=256, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=1024, out_features=1152, bias=False)\n",
              "          (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "          (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "        )\n",
              "        (mlp): Gemma3MLP(\n",
              "          (gate_proj): Linear4bit(in_features=1152, out_features=6912, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=1152, out_features=6912, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=6912, out_features=1152, bias=False)\n",
              "          (act_fn): PytorchGELUTanh()\n",
              "        )\n",
              "        (input_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
              "        (post_attention_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
              "        (pre_feedforward_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
              "        (post_feedforward_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
              "      )\n",
              "    )\n",
              "    (norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
              "    (rotary_emb): Gemma3RotaryEmbedding()\n",
              "    (rotary_emb_local): Gemma3RotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=1152, out_features=262144, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "qFkhXetGl8EM"
      },
      "outputs": [],
      "source": [
        "# ===================================================================\n",
        "# 2. BATCHED Helper Functions (Modified from your code)\n",
        "# ===================================================================\n",
        "def make_prompts(sentences: list[str]) -> list[str]:\n",
        "    \"\"\"Creates a list of prompts for a batch of sentences.\"\"\"\n",
        "    template = \"\"\"You are a grammar checker.\n",
        "Decide if the sentence below is grammatically correct.\n",
        "Answer with only one word: \"Correct\" or \"Incorrect\".\n",
        "\n",
        "Sentence: \"{sentence}\"\n",
        "Answer:\"\"\"\n",
        "    return [template.format(sentence=s) for s in sentences]\n",
        "\n",
        "def check_sentences_batched(sentences: list[str]) -> list[str]:\n",
        "    \"\"\"\n",
        "    Checks a BATCH of sentences for grammatical correctness.\n",
        "    \"\"\"\n",
        "    # 1. Create prompts for the whole batch\n",
        "    prompts = make_prompts(sentences)\n",
        "\n",
        "    # 2. Tokenize the entire batch at once\n",
        "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
        "\n",
        "    # 3. Generate outputs for the entire batch\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=5,\n",
        "            temperature=0.0,\n",
        "            do_sample=False\n",
        "        )\n",
        "\n",
        "    # 4. Decode all results at once\n",
        "    generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "    # 5. Parse the answer for each result in the batch\n",
        "    batch_results = []\n",
        "    for text in generated_texts:\n",
        "        try:\n",
        "            answer = text.split(\"Answer:\")[-1].strip().split()[0]\n",
        "            if answer.lower().startswith(\"c\"):\n",
        "                batch_results.append(\"Correct\")\n",
        "            else:\n",
        "                batch_results.append(\"Incorrect\")\n",
        "        except IndexError:\n",
        "            # Handle cases where the model might fail to generate a proper answer\n",
        "            batch_results.append(\"Error\")\n",
        "\n",
        "    return batch_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXJQDPHcm0RU",
        "outputId": "d3e6f92a-0827-461c-fdfb-dfa9d97ca742"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 89134 sentences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Checking grammar in batches:   0%|          | 0/2786 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Checking grammar in batches: 100%|██████████| 2786/2786 [1:03:03<00:00,  1.36s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✅ Done! Saved to sentences_with_grammar_batched.csv\n"
          ]
        }
      ],
      "source": [
        "# ===================================================================\n",
        "# 3. Load dataset and run the batched check\n",
        "# ===================================================================\n",
        "try:\n",
        "    # Use your specified column name 'sentences'\n",
        "    df = pd.read_csv(\"sentences.csv\")\n",
        "    df.dropna(subset=['sentences'], inplace=True)\n",
        "    sentences_to_check = df['sentences'].tolist()\n",
        "    print(f\"Loaded {len(sentences_to_check)} sentences.\")\n",
        "\n",
        "    BATCH_SIZE = 32  # You can adjust this based on your GPU memory\n",
        "    results = []\n",
        "\n",
        "    # Use tqdm to create a progress bar for the BATCHES\n",
        "    for i in tqdm(range(0, len(sentences_to_check), BATCH_SIZE), desc=\"Checking grammar in batches\"):\n",
        "        batch = sentences_to_check[i : i + BATCH_SIZE]\n",
        "        results.extend(check_sentences_batched(batch))\n",
        "\n",
        "    df[\"grammar_check\"] = results\n",
        "\n",
        "    # Save the final results\n",
        "    df.to_csv(\"sentences_with_grammar_batched.csv\", index=False)\n",
        "    print(\"\\n✅ Done! Saved to sentences_with_grammar_batched.csv\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"❌ Error: 'sentences.csv' not found. Please make sure the file is uploaded.\")\n",
        "except KeyError:\n",
        "    print(\"❌ Error: The CSV file must have a column named 'sentences'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "PRlRFGhJpClt",
        "outputId": "06ba0c5e-34d5-4f16-b7b5-df058d3ea024"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Success! Filtered data without the 'grammar_check' column has been saved to 'Gramitically_correct_dataset.csv'\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the names of your input and output files\n",
        "input_filename = 'sentences_with_grammar_batched.csv'\n",
        "output_filename = 'Gramitically_correct_dataset.csv'\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(input_filename)\n",
        "    filtered_df = df[df['grammar_check'] == 'Correct']\n",
        "    final_df = filtered_df[['uid', 'sentences']].copy()\n",
        "    final_df.to_csv(output_filename, index=False)\n",
        "\n",
        "    print(f\"✅ Success! Filtered data without the 'grammar_check' column has been saved to '{output_filename}'\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ Error: The file '{input_filename}' was not found. Please make sure it's in the same directory as the script.\")\n",
        "except KeyError as e:\n",
        "    print(f\"❌ Error: A required column was not found. Please check your column headers. Details: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
