{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf939596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Cleaning CSV based on videos in 'D:\\Des646\\iSign-videos_v1.1' ---\n",
      "Loaded 27305 rows from 'Gramitically_correct_dataset.csv'.\n",
      "Checking for missing videos and filtering the data...\n",
      "\n",
      "--- ‚úÖ Process Complete ---\n",
      "Original rows: 27305\n",
      "Rows with existing videos: 27066\n",
      "Rows deleted due to missing videos: 239\n",
      "\n",
      "Cleaned data has been saved to 'Gramitically_correct_videos_exist.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "# The CSV file you want to clean.\n",
    "INPUT_CSV = 'Gramitically_correct_dataset.csv'\n",
    "\n",
    "# The new CSV file that will be created with the cleaned data.\n",
    "OUTPUT_CSV = 'Gramitically_correct_videos_exist.csv'\n",
    "\n",
    "# The folder where all the videos are located.\n",
    "VIDEO_FOLDER = r\"D:\\Des646\\iSign-videos_v1.1\"\n",
    "\n",
    "# The file extension of your video files.\n",
    "VIDEO_EXTENSION = \".mp4\"\n",
    "# ---------------------\n",
    "\n",
    "print(f\"--- Cleaning CSV based on videos in '{VIDEO_FOLDER}' ---\")\n",
    "\n",
    "# 1. Load the CSV file\n",
    "try:\n",
    "    df = pd.read_csv(INPUT_CSV)\n",
    "    initial_count = len(df)\n",
    "    print(f\"Loaded {initial_count} rows from '{INPUT_CSV}'.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå Error: The file '{INPUT_CSV}' was not found.\")\n",
    "    exit()\n",
    "\n",
    "# 2. Define a function to check if a video file exists for a given UID\n",
    "def video_exists(uid):\n",
    "    filename = str(uid) + VIDEO_EXTENSION\n",
    "    full_path = os.path.join(VIDEO_FOLDER, filename)\n",
    "    return os.path.exists(full_path)\n",
    "\n",
    "# 3. Apply the check to each row and filter the DataFrame\n",
    "print(\"Checking for missing videos and filtering the data...\")\n",
    "# The .apply method runs the 'video_exists' function on each UID\n",
    "# The result is a boolean (True/False) series used to filter the DataFrame\n",
    "rows_to_keep = df['uid'].apply(video_exists)\n",
    "filtered_df = df[rows_to_keep]\n",
    "\n",
    "final_count = len(filtered_df)\n",
    "deleted_count = initial_count - final_count\n",
    "\n",
    "# 4. Save the cleaned data to a new file\n",
    "filtered_df.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "# 5. Print the final summary report\n",
    "print(\"\\n--- ‚úÖ Process Complete ---\")\n",
    "print(f\"Original rows: {initial_count}\")\n",
    "print(f\"Rows with existing videos: {final_count}\")\n",
    "print(f\"Rows deleted due to missing videos: {deleted_count}\")\n",
    "print(f\"\\nCleaned data has been saved to '{OUTPUT_CSV}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5a41723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Part 1: Finding 'Perfect' Sentences from 'Gramitically_correct_videos_exist.csv' ---\n",
      "Defined a Core Vocabulary of the top 250 words.\n",
      "Found 443 'perfect' sentences that only use the core vocabulary.\n",
      "\n",
      "--- Part 2: Cleaning the 'Perfect' Sentences ---\n",
      "Found 32 words with frequency < 2 within the perfect set. Removing them...\n",
      "‚úÖ Created a final clean 'seed' dataset of 417 sentences.\n",
      "\n",
      "--- Part 3: Augmenting the 'Seed' Dataset ---\n",
      "Calculated rarity scores to prioritize sentences with less common words.\n",
      "Generating 3000 new recombined sentences...\n",
      "‚úÖ Generated 3000 new training examples.\n",
      "\n",
      "--- Part 4: Saving the final files ---\n",
      "‚úÖ Original clean sentences saved to 'real_sentences.csv'\n",
      "‚úÖ Recombined sentences saved to 'recombined_sentences.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# --- NLTK setup (run once) ---\n",
    "try:\n",
    "    stopwords.words('english')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "# --------------------------------\n",
    "\n",
    "# --- Parameters You Can Tune ---\n",
    "# The ONLY input file needed: your large dataset of grammatically correct sentences.\n",
    "INPUT_FILENAME = 'Gramitically_correct_videos_exist.csv'\n",
    "\n",
    "# --- Stage 1 Parameters ---\n",
    "# Defines the initial vocabulary from the most common words in the large input file.\n",
    "CORE_VOCAB_SIZE = 250\n",
    "\n",
    "# --- Stage 2 Parameters ---\n",
    "# The minimum frequency for a word to be included in the final \"real\" sentences.\n",
    "MIN_CORE_FREQUENCY = 2\n",
    "\n",
    "# --- Stage 3 Parameters ---\n",
    "# The number of new, recombined sentences you want to create.\n",
    "NUM_AUGMENTED_EXAMPLES = 3000\n",
    "\n",
    "# Names for the two final output files.\n",
    "OUTPUT_REAL_SENTENCES = 'real_sentences.csv'\n",
    "OUTPUT_RECOMBINED_SENTENCES = 'recombined_sentences.csv'\n",
    "# -----------------------------------\n",
    "\n",
    "def preprocess_text(sentence):\n",
    "    \"\"\"Cleans and tokenizes a sentence.\"\"\"\n",
    "    if not isinstance(sentence, str): return []\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r'[^\\w\\s]', '', sentence)\n",
    "    tokens = word_tokenize(sentence)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "\n",
    "# --- Part 1: Finding \"Perfect\" Sentences from the Main File ---\n",
    "print(f\"--- Part 1: Finding 'Perfect' Sentences from '{INPUT_FILENAME}' ---\")\n",
    "try:\n",
    "    df = pd.read_csv(INPUT_FILENAME)\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå Error: Input file '{INPUT_FILENAME}' not found. Please make sure it's in the same directory.\")\n",
    "    exit()\n",
    "\n",
    "df['tokens'] = df['sentences'].apply(preprocess_text)\n",
    "df = df[df['tokens'].str.len() > 0].copy()\n",
    "\n",
    "# Define the Core Vocabulary from the most common words in the entire dataset\n",
    "all_words = [word for tokens in df['tokens'] for word in tokens]\n",
    "word_counts = Counter(all_words)\n",
    "core_vocab = {word for word, count in word_counts.most_common(CORE_VOCAB_SIZE)}\n",
    "print(f\"Defined a Core Vocabulary of the top {len(core_vocab)} words.\")\n",
    "\n",
    "# Find all sentences composed ONLY of these core words\n",
    "def is_in_core_vocab(tokens):\n",
    "    return all(word in core_vocab for word in tokens)\n",
    "perfect_sentences_df = df[df['tokens'].apply(is_in_core_vocab)].copy()\n",
    "print(f\"Found {len(perfect_sentences_df)} 'perfect' sentences that only use the core vocabulary.\")\n",
    "\n",
    "\n",
    "# --- Part 2: Cleaning the \"Perfect\" Sentences to Create the \"Seed\" ---\n",
    "print(f\"\\n--- Part 2: Cleaning the 'Perfect' Sentences ---\")\n",
    "# Count words within this \"perfect\" set to find the very rare ones\n",
    "perfect_word_counts = Counter(word for tokens in perfect_sentences_df['tokens'] for word in tokens)\n",
    "rare_words = {word for word, count in perfect_word_counts.items() if count < MIN_CORE_FREQUENCY}\n",
    "print(f\"Found {len(rare_words)} words with frequency < {MIN_CORE_FREQUENCY} within the perfect set. Removing them...\")\n",
    "\n",
    "# Create the final, clean \"seed\" DataFrame\n",
    "pure_core_df = perfect_sentences_df[~perfect_sentences_df['tokens'].apply(lambda t: any(w in rare_words for w in t))].copy()\n",
    "pure_core_df = pure_core_df.dropna(subset=['uid', 'sentences'])\n",
    "print(f\"‚úÖ Created a final clean 'seed' dataset of {len(pure_core_df)} sentences.\")\n",
    "\n",
    "\n",
    "# --- Part 3: Augmenting the \"Seed\" via Weighted Recombination ---\n",
    "print(f\"\\n--- Part 3: Augmenting the 'Seed' Dataset ---\")\n",
    "if len(pure_core_df) < 2:\n",
    "    print(\"‚ùå Error: The clean 'seed' dataset has fewer than 2 sentences, cannot perform recombination.\")\n",
    "    exit()\n",
    "    \n",
    "# Calculate word frequencies and rarity scores within the PURE dataset\n",
    "word_counts_core = Counter(word for tokens in pure_core_df['tokens'] for word in tokens)\n",
    "def calculate_rarity_score(tokens):\n",
    "    if not tokens: return 0\n",
    "    return sum(1 / (word_counts_core.get(word, 1)) for word in tokens)\n",
    "\n",
    "pure_core_df['rarity_score'] = pure_core_df['tokens'].apply(calculate_rarity_score)\n",
    "print(\"Calculated rarity scores to prioritize sentences with less common words.\")\n",
    "\n",
    "# Generate new augmented examples\n",
    "print(f\"Generating {NUM_AUGMENTED_EXAMPLES} new recombined sentences...\")\n",
    "augmented_data = []\n",
    "for i in range(NUM_AUGMENTED_EXAMPLES):\n",
    "    # --- FIX IS HERE ---\n",
    "    # Sample two rows into a single DataFrame\n",
    "    sampled_blocks = pure_core_df.sample(n=2, weights='rarity_score', replace=True)\n",
    "    \n",
    "    # Get the first and second rows from the sampled DataFrame\n",
    "    block_A_data = sampled_blocks.iloc[0]\n",
    "    block_B_data = sampled_blocks.iloc[1]\n",
    "    \n",
    "    # Extract the text and UIDs from each row\n",
    "    text_A, uid_A = block_A_data['sentences'], block_A_data['uid']\n",
    "    text_B, uid_B = block_B_data['sentences'], block_B_data['uid']\n",
    "    # --- END FIX ---\n",
    "    \n",
    "    new_text = text_A + \" \" + text_B\n",
    "    augmented_data.append({'uid1': uid_A, 'uid2': uid_B, 'text': new_text})\n",
    "print(f\"‚úÖ Generated {len(augmented_data)} new training examples.\")\n",
    "\n",
    "\n",
    "# --- Part 4: Saving the Final Outputs ---\n",
    "print(\"\\n--- Part 4: Saving the final files ---\")\n",
    "pure_core_df[['uid', 'sentences']].to_csv(OUTPUT_REAL_SENTENCES, index=False)\n",
    "print(f\"‚úÖ Original clean sentences saved to '{OUTPUT_REAL_SENTENCES}'\")\n",
    "\n",
    "recombined_df = pd.DataFrame(augmented_data)\n",
    "recombined_df.to_csv(OUTPUT_RECOMBINED_SENTENCES, index=False)\n",
    "print(f\"‚úÖ Recombined sentences saved to '{OUTPUT_RECOMBINED_SENTENCES}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec88387",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ed5f0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning for videos in D:\\Des646\\Dataset_Creation\\final_videos...\n",
      "Found 330 existing video files.\n",
      "\n",
      "Processing real_sentences.csv...\n",
      "Finished processing real_sentences.csv.\n",
      "  Initial rows: 417\n",
      "  Final rows:   330\n",
      "  Rows removed: 87\n",
      "\n",
      "Processing recombined_sentences.csv...\n",
      "Finished processing recombined_sentences.csv.\n",
      "  Initial rows: 3000\n",
      "  Final rows:   2022\n",
      "  Rows removed: 978\n",
      "\n",
      "Script finished.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# --- Configuration ---\n",
    "# PLEASE UPDATE THESE PATHS IF THEY ARE INCORRECT\n",
    "VIDEOS_DIR = r\"D:\\Des646\\Dataset_Creation\\final_videos\"\n",
    "REAL_CSV_PATH = r\"D:\\Des646\\Dataset_Creation\\real_sentences.csv\"\n",
    "RECOMBINED_CSV_PATH = r\"D:\\Des646\\Dataset_Creation\\recombined_sentences.csv\"\n",
    "# --- End of Configuration ---\n",
    "\n",
    "def clean_csv_files():\n",
    "    \"\"\"\n",
    "    Scans the video directory and filters two CSV files based on\n",
    "    the existence of corresponding video files.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. Define paths\n",
    "        videos_path = Path(VIDEOS_DIR)\n",
    "        real_csv = Path(REAL_CSV_PATH)\n",
    "        recombined_csv = Path(RECOMBINED_CSV_PATH)\n",
    "\n",
    "        # 2. Check if paths exist\n",
    "        if not videos_path.is_dir():\n",
    "            print(f\"Error: Video directory not found at: {videos_path}\")\n",
    "            return\n",
    "        if not real_csv.is_file():\n",
    "            print(f\"Error: Real sentences CSV not found at: {real_csv}\")\n",
    "            return\n",
    "        if not recombined_csv.is_file():\n",
    "            print(f\"Error: Recombined sentences CSV not found at: {recombined_csv}\")\n",
    "            return\n",
    "\n",
    "        # 3. Get all existing video UIDs from the folder\n",
    "        print(f\"Scanning for videos in {videos_path}...\")\n",
    "        \n",
    "        # Use .stem to get the filename without the .mp4 extension\n",
    "        existing_video_uids = {file.stem for file in videos_path.glob('*.mp4')}\n",
    "        \n",
    "        if not existing_video_uids:\n",
    "            print(f\"Warning: No .mp4 files found in {videos_path}. Both CSVs will be emptied.\")\n",
    "        else:\n",
    "            print(f\"Found {len(existing_video_uids)} existing video files.\")\n",
    "\n",
    "        # --- 4. Process real_sentences.csv ---\n",
    "        print(f\"\\nProcessing {real_csv.name}...\")\n",
    "        try:\n",
    "            df_real = pd.read_csv(real_csv, dtype={'uid': str}) # Read uid as string\n",
    "            initial_real_rows = len(df_real)\n",
    "            \n",
    "            # Filter rows where 'uid' is in our set of existing video UIDs\n",
    "            df_real_filtered = df_real[df_real['uid'].isin(existing_video_uids)]\n",
    "            final_real_rows = len(df_real_filtered)\n",
    "            \n",
    "            # Save the changes back to the same file\n",
    "            df_real_filtered.to_csv(real_csv, index=False)\n",
    "            \n",
    "            print(f\"Finished processing {real_csv.name}.\")\n",
    "            print(f\"  Initial rows: {initial_real_rows}\")\n",
    "            print(f\"  Final rows:   {final_real_rows}\")\n",
    "            print(f\"  Rows removed: {initial_real_rows - final_real_rows}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {real_csv.name}: {e}\")\n",
    "\n",
    "        # --- 5. Process recombined_sentences.csv ---\n",
    "        print(f\"\\nProcessing {recombined_csv.name}...\")\n",
    "        try:\n",
    "            df_recombined = pd.read_csv(recombined_csv, dtype={'uid1': str, 'uid2': str})\n",
    "            initial_recombined_rows = len(df_recombined)\n",
    "            \n",
    "            # Filter rows where *both* uid1 AND uid2 are in the set\n",
    "            condition_uid1_exists = df_recombined['uid1'].isin(existing_video_uids)\n",
    "            condition_uid2_exists = df_recombined['uid2'].isin(existing_video_uids)\n",
    "            \n",
    "            df_recombined_filtered = df_recombined[condition_uid1_exists & condition_uid2_exists]\n",
    "            final_recombined_rows = len(df_recombined_filtered)\n",
    "            \n",
    "            # Save the changes back to the same file\n",
    "            df_recombined_filtered.to_csv(recombined_csv, index=False)\n",
    "            \n",
    "            print(f\"Finished processing {recombined_csv.name}.\")\n",
    "            print(f\"  Initial rows: {initial_recombined_rows}\")\n",
    "            print(f\"  Final rows:   {final_recombined_rows}\")\n",
    "            print(f\"  Rows removed: {initial_recombined_rows - final_recombined_rows}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {recombined_csv.name}: {e}\")\n",
    "\n",
    "        print(\"\\nScript finished.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # To run this script:\n",
    "    # 1. Make sure you have pandas installed: pip install pandas\n",
    "    # 2. Save this file as clean_csv_by_videos.py\n",
    "    # 3. Run it from your terminal: python clean_csv_by_videos.py\n",
    "    clean_csv_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06927273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Word Frequency Analysis ---\n",
      "‚úÖ Loaded 330 real sentences and 2022 recombined sentences.\n",
      "Analyzing a total of 2352 sentences.\n",
      "Processing text and tokenizing all sentences...\n",
      "\n",
      "Saving vocabulary to vocabulary.csv...\n",
      "‚úÖ Vocabulary file with 174 words saved successfully.\n",
      "\n",
      "--- üìä Final Dataset Analysis Report ---\n",
      "Total Unique Words (Vocabulary Size): 174\n",
      "Highest Frequency: 'know' appeared 331 times.\n",
      "Lowest Frequency: ¬†'sign' appeared 17 times.\n",
      "\n",
      "There are 1 word(s) with the lowest frequency of 17:\n",
      "['sign']\n",
      "\n",
      "--- Top 15 Most Common Words ---\n",
      "- know: 331 times\n",
      "- let: 281 times\n",
      "- people: 260 times\n",
      "- picture: 240 times\n",
      "- time: 235 times\n",
      "- shocking: 205 times\n",
      "- see: 200 times\n",
      "- case: 194 times\n",
      "- tell: 185 times\n",
      "- video: 184 times\n",
      "- deaf: 176 times\n",
      "- went: 169 times\n",
      "- first: 156 times\n",
      "- together: 144 times\n",
      "- however: 135 times\n",
      "\n",
      "--- Analysis Complete ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import sys\n",
    "\n",
    "# --- NLTK setup (run once if needed) ---\n",
    "try:\n",
    "    stopwords.words('english')\n",
    "except LookupError:\n",
    "    print(\"Downloading 'stopwords' from NLTK...\")\n",
    "    nltk.download('stopwords')\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    print(\"Downloading 'punkt' tokenizer from NLTK...\")\n",
    "    nltk.download('punkt')\n",
    "# --------------------------------\n",
    "\n",
    "# --- Configuration ---\n",
    "# The names of the two files generated by the previous script.\n",
    "REAL_SENTENCES_FILE = 'real_sentences.csv'\n",
    "RECOMBINED_SENTENCES_FILE = 'recombined_sentences2.csv'\n",
    "# The name of the new file to save the vocabulary to.\n",
    "VOCABULARY_FILE = 'vocabulary2.csv'\n",
    "# ---------------------\n",
    "\n",
    "def preprocess_text(sentence):\n",
    "    \"\"\"\n",
    "    Cleans and tokenizes a sentence.\n",
    "    This MUST be identical to the function used to create the datasets.\n",
    "    \"\"\"\n",
    "    if not isinstance(sentence, str): return []\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r'[^\\w\\s]', '', sentence) # Remove punctuation\n",
    "    tokens = word_tokenize(sentence)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [word for word in tokens if word not in stop_words and word.isalpha()] # Added isalpha() to remove numbers/stray ws\n",
    "\n",
    "# --- Main Analysis Logic ---\n",
    "print(\"--- Starting Word Frequency Analysis ---\")\n",
    "\n",
    "# 1. Load both datasets\n",
    "try:\n",
    "    real_df = pd.read_csv(REAL_SENTENCES_FILE)\n",
    "    recombined_df = pd.read_csv(RECOMBINED_SENTENCES_FILE)\n",
    "    print(f\"‚úÖ Loaded {len(real_df)} real sentences and {len(recombined_df)} recombined sentences.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ùå Error: Could not find a required file. Make sure both '{REAL_SENTENCES_FILE}' and '{RECOMBINED_SENTENCES_FILE}' are present.\")\n",
    "    print(f\"Details: {e}\")\n",
    "    sys.exit(1)\n",
    "except pd.errors.EmptyDataError as e:\n",
    "    print(f\"‚ùå Error: One of the files is empty. {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "# 2. Combine the text from both files into a single list\n",
    "real_sentences = real_df['sentences']\n",
    "recombined_sentences = recombined_df['text']\n",
    "all_sentences = pd.concat([real_sentences, recombined_sentences], ignore_index=True)\n",
    "\n",
    "print(f\"Analyzing a total of {len(all_sentences)} sentences.\")\n",
    "\n",
    "# 3. Preprocess all sentences and create a flat list of all words\n",
    "print(\"Processing text and tokenizing all sentences...\")\n",
    "# This creates a list of lists, where each inner list is the tokens of a sentence\n",
    "tokenized_sentences = all_sentences.apply(preprocess_text)\n",
    "\n",
    "# This flattens the list of lists into a single list of all words\n",
    "all_words = [word for sentence_tokens in tokenized_sentences for word in sentence_tokens]\n",
    "\n",
    "if not all_words:\n",
    "    print(\"‚ùå No words were found after processing. Cannot perform analysis.\")\n",
    "    sys.exit()\n",
    "\n",
    "# 4. Count the frequency of each word\n",
    "word_counts = Counter(all_words)\n",
    "\n",
    "# --- 5. NEW: Save vocabulary to CSV file ---\n",
    "print(f\"\\nSaving vocabulary to {VOCABULARY_FILE}...\")\n",
    "try:\n",
    "    # Convert the Counter object to a DataFrame\n",
    "    vocab_df = pd.DataFrame(word_counts.items(), columns=['word', 'freq'])\n",
    "    \n",
    "    # Sort by frequency in descending order\n",
    "    vocab_df = vocab_df.sort_values(by='freq', ascending=False)\n",
    "    \n",
    "    # Save to CSV\n",
    "    vocab_df.to_csv(VOCABULARY_FILE, index=False)\n",
    "    \n",
    "    print(f\"‚úÖ Vocabulary file with {len(vocab_df)} words saved successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error saving vocabulary file: {e}\")\n",
    "# ---------------------------------------------\n",
    "\n",
    "# 6. Generate and print the final report\n",
    "print(\"\\n--- üìä Final Dataset Analysis Report ---\")\n",
    "print(f\"Total Unique Words (Vocabulary Size): {len(word_counts)}\")\n",
    "\n",
    "# Find the lowest and highest frequencies\n",
    "most_common_word, highest_freq = word_counts.most_common(1)[0]\n",
    "least_common_word, lowest_freq = word_counts.most_common()[-1]\n",
    "\n",
    "print(f\"Highest Frequency: '{most_common_word}' appeared {highest_freq} times.\")\n",
    "print(f\"Lowest Frequency: ¬†'{least_common_word}' appeared {lowest_freq} times.\")\n",
    "\n",
    "# Find all words that share the lowest frequency\n",
    "words_with_lowest_freq = [word for word, count in word_counts.items() if count == lowest_freq]\n",
    "\n",
    "print(f\"\\nThere are {len(words_with_lowest_freq)} word(s) with the lowest frequency of {lowest_freq}:\")\n",
    "# Print a sample of them if there are too many\n",
    "print(words_with_lowest_freq[:20]) # Shows up to the first 20\n",
    "\n",
    "print(\"\\n--- Top 15 Most Common Words ---\")\n",
    "for word, count in word_counts.most_common(15):\n",
    "    print(f\"- {word}: {count} times\")\n",
    "\n",
    "print(\"\\n--- Analysis Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f315f044",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
